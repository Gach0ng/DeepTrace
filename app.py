import streamlit as st
import pandas as pd
import psycopg2
import hanlp
import re
import time
from datetime import datetime
from streamlit_agraph import agraph, Node, Edge, Config
import plotly.express as px

# ==========================================
# 1. ç³»ç»Ÿé…ç½®
# ==========================================
st.set_page_config(
    page_title="DeepTrace | æƒ…æŠ¥çº¿ç´¢åˆ†æç³»ç»Ÿ",
    layout="wide",
    page_icon="ğŸ¦…",
    initial_sidebar_state="collapsed"
)

st.markdown("""
    <style>
        header, footer, #MainMenu {visibility: hidden;}
        .block-container {
            padding-top: 2rem !important;
            padding-left: 2rem !important;
            padding-right: 2rem !important;
            max-width: 100% !important;
        }
        .filter-container {
            background-color: #ffffff;
            border-bottom: 2px solid #f0f2f6;
            padding: 15px 10px 10px 10px;
            margin-bottom: 20px;
            position: sticky;
            top: 0;
            z-index: 999;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        }
        div[data-testid="stMetric"] {
            background-color: #f8fafc;
            border: 1px solid #e2e8f0;
            padding: 15px;
            border-radius: 8px;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®åº“é…ç½®
# ==========================================
DB_CONFIG = {
    'dbname': 'Test',
    'user': 'postgres',
    'password': 'root',
    'host': 'localhost',
    'port': '5432'
}


def get_db_conn():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception:
        return None


# ==========================================
# 3. æ ¸å¿ƒé€»è¾‘ (NLP & DB Init)
# ==========================================
@st.cache_resource
def load_nlp_model():
    try:
        with st.spinner('æ­£åœ¨åŠ è½½ NLP ç¥ç»å…ƒç½‘ç»œ...'):
            tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
            ner = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ELECTRA_SMALL_ZH)
        return tok, ner
    except:
        return None, None


def init_db_structure():
    conn = get_db_conn()
    if not conn: return
    try:
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS t_clues (
                id SERIAL PRIMARY KEY, source_email VARCHAR(150), batch_no VARCHAR(100),
                send_time TIMESTAMP, content TEXT, subject VARCHAR(255), recorder VARCHAR(100),
                remarks TEXT, original_file VARCHAR(255), process_status SMALLINT DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, org VARCHAR(200)
            );
        """)
        cur.execute("ALTER TABLE t_clues ADD COLUMN IF NOT EXISTS org VARCHAR(200);")
        cur.execute("""
            CREATE TABLE IF NOT EXISTS t_entities (
                id SERIAL PRIMARY KEY, name VARCHAR(200) NOT NULL, type VARCHAR(50) NOT NULL,
                UNIQUE(name, type)
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS t_relations (
                clue_id INT REFERENCES t_clues(id), entity_id INT REFERENCES t_entities(id),
                PRIMARY KEY (clue_id, entity_id)
            );
        """)
        conn.commit()
        conn.close()
    except Exception as e:
        st.error(f"DB Init Error: {e}")


tok, ner = load_nlp_model()
init_db_structure()


# ==========================================
# 4. æ•°æ®ç®¡é“
# ==========================================
def save_excel_to_db(uploaded_file):
    conn = get_db_conn()
    if not conn: return 0
    try:
        df = pd.read_excel(uploaded_file)
        col_map = {
            'æ¥æºé‚®ç®±': 'source_email', 'å‘ä»¶äºº': 'source_email', 'é‚®ç®±': 'source_email',
            'æ‰¹æ¬¡': 'batch_no', 'æ”¶å‘æ—¥æœŸ': 'send_time', 'æ—¶é—´': 'send_time',
            'é‚®ä»¶å†…å®¹': 'content', 'æ­£æ–‡': 'content', 'é‚®ä»¶å': 'subject', 'æ ‡é¢˜': 'subject', 'ä¸»é¢˜': 'subject',
            'è®°å½•äºº': 'recorder', 'å¤‡æ³¨': 'remarks', 'åŸä»¶å': 'original_file', 'æœºæ„': 'org'
        }
        df.rename(columns=col_map, inplace=True)
        cur = conn.cursor()
        count = 0
        for _, row in df.iterrows():
            send_time = row.get('send_time') if pd.notna(row.get('send_time')) else datetime.now()
            cur.execute("""
                INSERT INTO t_clues (source_email, batch_no, send_time, content, subject, recorder, remarks, original_file, process_status, org)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, 0, %s)
            """, (
                row.get('source_email'), row.get('batch_no'), send_time,
                row.get('content'), row.get('subject'), row.get('recorder'),
                row.get('remarks'), row.get('original_file'), row.get('org')
            ))
            count += 1
        conn.commit()
        conn.close()
        get_org_options.clear()
        get_time_options_by_org.clear()
        get_analytics_data.clear()
        return count
    except Exception:
        return 0


def run_analysis_pipeline():
    if not tok or not ner: return 0
    conn = get_db_conn()
    if not conn: return 0
    cur = conn.cursor()
    cur.execute("SELECT id, content, subject, source_email FROM t_clues WHERE process_status = 0")
    rows = cur.fetchall()
    if not rows:
        conn.close()
        return 0

    processed_count = 0
    bar = st.progress(0)
    for i, row in enumerate(rows):
        cid, content, subject, email = row
        text = f"{subject or ''} {content or ''} {email or ''}"
        entities = set()
        try:
            for term, label in ner(tok(text), tasks='ner*'):
                if len(term) > 1 and label in ['PERSON', 'PER', 'ORG', 'ORGANIZATION', 'LOC', 'LOCATION']:
                    std_label = 'äººå' if label in ['PERSON', 'PER'] else 'æœºæ„' if label in ['ORG',
                                                                                              'ORGANIZATION'] else 'åœ°å'
                    entities.add((term, std_label))
            phones = re.findall(r'(?<!\d)1[3-9]\d{9}(?!\d)', text)
            for p in phones: entities.add((p, 'æ‰‹æœºå·'))

            for name, etype in entities:
                cur.execute(
                    "INSERT INTO t_entities (name, type) VALUES (%s, %s) ON CONFLICT (name, type) DO UPDATE SET name=EXCLUDED.name RETURNING id",
                    (name, etype))
                eid = cur.fetchone()[0]
                cur.execute("INSERT INTO t_relations (clue_id, entity_id) VALUES (%s, %s) ON CONFLICT DO NOTHING",
                            (cid, eid))
            cur.execute("UPDATE t_clues SET process_status = 1 WHERE id = %s", (cid,))
        except:
            cur.execute("UPDATE t_clues SET process_status = -1 WHERE id = %s", (cid,))
        processed_count += 1
        bar.progress((i + 1) / len(rows))
    conn.commit()
    conn.close()
    get_analytics_data.clear()
    return processed_count


# ==========================================
# 5. æ•°æ®æŸ¥è¯¢ (Cache)
# ==========================================
@st.cache_data(ttl=600)
def get_org_options():
    conn = get_db_conn()
    if not conn: return ["å…¨éƒ¨æœºæ„"]
    try:
        df = pd.read_sql("SELECT DISTINCT org FROM t_clues WHERE org IS NOT NULL AND org != '' ORDER BY org", conn)
        conn.close()
        return ["å…¨éƒ¨æœºæ„"] + df['org'].tolist()
    except:
        return ["å…¨éƒ¨æœºæ„"]


# æ¢å¤ï¼šæ ¹æ®æœºæ„ç­›é€‰æ—¶é—´å­—ç¬¦ä¸²åˆ—è¡¨
@st.cache_data(ttl=600)
def get_time_options_by_org(selected_org):
    conn = get_db_conn()
    if not conn: return ["å…¨éƒ¨æ—¶é—´"]
    try:
        sql = "SELECT DISTINCT TO_CHAR(send_time, 'YYYY-MM-DD') as d FROM t_clues WHERE send_time IS NOT NULL"
        params = []
        if selected_org != "å…¨éƒ¨æœºæ„":
            sql += " AND org = %s"
            params.append(selected_org)
        sql += " ORDER BY d DESC"
        df = pd.read_sql(sql, conn, params=params)
        conn.close()
        return ["å…¨éƒ¨æ—¶é—´"] + df['d'].tolist()
    except:
        return ["å…¨éƒ¨æ—¶é—´"]


@st.cache_data(ttl=300)
def get_analytics_data(keyword, org, date_val):
    conn = get_db_conn()
    if not conn: return None

    conditions = ["1=1"]
    params = []

    if org != "å…¨éƒ¨æœºæ„":
        conditions.append("c.org = %s")
        params.append(org)

    # æ¢å¤ï¼šç²¾ç¡®åŒ¹é…æŸä¸€å¤© æˆ– å…¨éƒ¨æ—¶é—´
    if date_val != "å…¨éƒ¨æ—¶é—´":
        conditions.append("TO_CHAR(c.send_time, 'YYYY-MM-DD') = %s")
        params.append(date_val)

    if keyword:
        conditions.append("(c.content LIKE %s OR c.subject LIKE %s OR e.name LIKE %s)")
        wildcard = f'%{keyword}%'
        params.extend([wildcard, wildcard, wildcard])

    where_clause = " AND ".join(conditions)
    data = {}

    try:
        sql_clues = f"""
            SELECT DISTINCT c.id, c.subject, c.send_time, c.org, c.source_email, c.content
            FROM t_clues c
            LEFT JOIN t_relations r ON c.id = r.clue_id
            LEFT JOIN t_entities e ON r.entity_id = e.id
            WHERE {where_clause}
            ORDER BY c.send_time DESC LIMIT 300
        """
        data['clues'] = pd.read_sql(sql_clues, conn, params=params)

        if not data['clues'].empty:
            ids = tuple(data['clues']['id'].tolist())
            if len(ids) == 1: ids = (ids[0], ids[0])

            sql_ent = f"""
                SELECT e.name, e.type, COUNT(*) as weight
                FROM t_entities e
                JOIN t_relations r ON e.id = r.entity_id
                WHERE r.clue_id IN {ids}
                GROUP BY e.name, e.type
                ORDER BY weight DESC LIMIT 100
            """
            data['entities'] = pd.read_sql(sql_ent, conn)

            top_ids = tuple(data['clues']['id'].head(50).tolist())
            if len(top_ids) == 1: top_ids = (top_ids[0], top_ids[0])
            sql_rel = f"""
                SELECT r.clue_id, e.id as eid, e.name, e.type 
                FROM t_relations r JOIN t_entities e ON r.entity_id = e.id 
                WHERE r.clue_id IN {top_ids} LIMIT 500
            """
            data['relations'] = pd.read_sql(sql_rel, conn)
        else:
            data['entities'] = pd.DataFrame()
            data['relations'] = pd.DataFrame()

        conn.close()
        return data
    except Exception:
        conn.close()
        return None


def get_node_detail(node_id):
    conn = get_db_conn()
    if not conn or not node_id: return None
    cur = conn.cursor()
    info = {}
    try:
        if node_id.startswith("MAIL_"):
            cid = node_id.split("_")[1]
            cur.execute("SELECT subject, send_time, source_email, org, content FROM t_clues WHERE id=%s", (cid,))
            row = cur.fetchone()
            if row:
                info = {
                    "type": "mail", "title": row[0],
                    "meta": [("ğŸ“… æ—¶é—´", str(row[1])[:19]), ("ğŸ¢ æœºæ„", row[3]), ("ğŸ“§ å‘ä»¶äºº", row[2])],
                    "body": row[4]
                }
        elif node_id.startswith("ENT_"):
            eid = node_id.split("_")[1]
            cur.execute("SELECT name, type FROM t_entities WHERE id=%s", (eid,))
            row = cur.fetchone()
            if row:
                info = {"type": "entity", "title": row[0], "meta": [("ğŸ·ï¸ ç±»å‹", row[1])], "body": None}
    except:
        pass
    conn.close()
    return info


# ==========================================
# 6. å‰ç«¯ UI æ„å»º
# ==========================================
st.title("ğŸ¦… DeepTrace | æƒ…æŠ¥çº¿ç´¢åˆ†æç³»ç»Ÿ")

# --- A. æ•°æ®ç®¡ç†åŒº ---
with st.expander("ğŸ“‚ æ•°æ®ç®¡ç†ä¸­å¿ƒ (å±•å¼€/æ”¶èµ·)", expanded=True):
    col_admin1, col_admin2 = st.columns([1, 1])
    with col_admin1:
        st.markdown("#### ğŸ“¥ çº¿ç´¢å…¥åº“")
        up_file = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶", type="xlsx", label_visibility="collapsed")
        if up_file and st.button("ç¡®è®¤å¯¼å…¥", type="primary"):
            n = save_excel_to_db(up_file)
            if n:
                st.success(f"æˆåŠŸå…¥åº“ {n} æ¡ï¼")
                time.sleep(1)
                st.rerun()

    with col_admin2:
        st.markdown("#### ğŸ§  æ™ºèƒ½åˆ†æçŠ¶æ€")
        pending_count = 0
        conn_check = get_db_conn()
        if conn_check:
            try:
                curr = conn_check.cursor()
                curr.execute("SELECT COUNT(*) FROM t_clues WHERE process_status = 0")
                pending_count = curr.fetchone()[0]
            except:
                pass
            finally:
                conn_check.close()

        if pending_count > 0:
            st.warning(f"âš ï¸ {pending_count} æ¡çº¿ç´¢å¾…åˆ†æ")
            if st.button(f"ğŸš€ ç«‹å³è¿è¡Œ AI åˆ†æ ({pending_count})", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨æå–å®ä½“å…³ç³»..."):
                    n = run_analysis_pipeline()
                st.success(f"å®Œæˆï¼æ–°å¢ {n} ç»„å…³ç³»")
                time.sleep(1)
                st.rerun()
        else:
            st.success("âœ… ç³»ç»Ÿå°±ç»ª")
            if st.button("ğŸ”„ å¼ºåˆ¶é‡æ‰«"):
                run_analysis_pipeline()
                st.rerun()

# --- B. æ‚¬æµ®ç­›é€‰æ¡ (æ¢å¤ SelectBox) ---
st.markdown('<div class="filter-container">', unsafe_allow_html=True)
c1, c2, c3, c4 = st.columns([1.5, 1.5, 3, 1])

with c1:
    org_list = get_org_options()
    sel_org = st.selectbox("ğŸ¢ å½’å±æœºæ„", org_list)

with c2:
    # æ¢å¤ï¼šä¸‹æ‹‰é€‰æ‹©æ—¶é—´
    time_list = get_time_options_by_org(sel_org)
    sel_time = st.selectbox("ğŸ“… æ—¶é—´èŠ‚ç‚¹", time_list)

with c3:
    search_keyword = st.text_input("ğŸ” å…¨å±€ä¾¦æŸ¥", placeholder="è¾“å…¥çº¿ç´¢å†…å®¹ / äººå / é‚®ç®±...")

with c4:
    st.write("")
    st.write("")
    start_search = st.button("ğŸš€ å¼€å§‹ä¾¦æŸ¥", use_container_width=True, type="primary")

st.markdown('</div>', unsafe_allow_html=True)

# --- C. æ•°æ®åŠ è½½ ---
if 'selected_node_id' not in st.session_state:
    st.session_state.selected_node_id = None
if 'analytics_data' not in st.session_state:
    st.session_state.analytics_data = None

if start_search or st.session_state.analytics_data is None:
    with st.spinner("æ­£åœ¨æ„å»ºæƒ…æŠ¥ç½‘ç»œ..."):
        st.session_state.analytics_data = get_analytics_data(search_keyword, sel_org, sel_time)
        st.session_state.selected_node_id = None

data_bundle = st.session_state.analytics_data
df_clues = data_bundle['clues'] if data_bundle else pd.DataFrame()
df_ents = data_bundle['entities'] if data_bundle else pd.DataFrame()
df_rels = data_bundle.get('relations', pd.DataFrame()) if data_bundle else pd.DataFrame()

if df_clues.empty:
    st.info("ğŸ‘‹ æš‚æ— æ•°æ®ï¼Œè¯·æ£€æŸ¥ç­›é€‰æ¡ä»¶ã€‚")
    st.stop()

# æ ¸å¿ƒæŒ‡æ ‡
m1, m2, m3, m4 = st.columns(4)
m1.metric("å‘½ä¸­çº¿ç´¢", f"{len(df_clues)}")
m2.metric("æ¶‰åŠå®ä½“", f"{len(df_ents)}")
if not df_clues.empty:
    times = pd.to_datetime(df_clues['send_time'])
    m3.metric("æ—¶é—´èŒƒå›´", f"{times.min():%m-%d} ~ {times.max():%m-%d}")
    top_u = df_clues['source_email'].mode()[0] if not df_clues['source_email'].empty else "N/A"
    m4.metric("æ ¸å¿ƒäººç‰©", str(top_u)[:15] + ".." if len(str(top_u)) > 15 else str(top_u))

st.markdown("---")

tab_dash, tab_graph, tab_time, tab_ent = st.tabs(["ğŸ“Š ç»Ÿè®¡çœ‹æ¿", "ğŸ•¸ï¸ å…³è”ä¾¦æŸ¥", "ğŸ“… æ—¶åºåˆ†æ", "ğŸ‘¥ å®ä½“æ˜ç»†"])

with tab_dash:
    c1, c2 = st.columns(2)
    with c1:
        st.caption("å‘ä»¶äººæ´»è·ƒåº¦ TOP10")
        if not df_clues.empty:
            df_top = df_clues['source_email'].value_counts().reset_index().head(10)
            df_top.columns = ['email', 'count']
            st.plotly_chart(px.bar(df_top, x='count', y='email', orientation='h'), use_container_width=True)
    with c2:
        st.caption("å®ä½“å…³é”®è¯åˆ†å¸ƒ")
        if not df_ents.empty:
            st.plotly_chart(px.treemap(df_ents, path=['type', 'name'], values='weight'), use_container_width=True)

# === æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ï¼šå¢å¼ºå›¾è°±é…ç½®ï¼Œç¡®ä¿å±…ä¸­æ˜¾ç¤º ===
with tab_graph:
    cg1, cg2 = st.columns([3, 1])
    with cg1:
        st.markdown("#### äº¤äº’å¼å›¾è°±")
        nodes, edges = [], []
        exist_ids = set()

        if not df_clues.empty:
            # ä¼˜å…ˆå±•ç¤ºå‰ 30 æ¡çº¿ç´¢ï¼Œä¿è¯æ€§èƒ½
            for _, row in df_clues.head(30).iterrows():
                nid = f"MAIL_{row['id']}"
                if nid not in exist_ids:
                    label = row['subject'][:6] + ".." if row['subject'] and len(row['subject']) > 6 else "æ— é¢˜"
                    nodes.append(
                        Node(id=nid, label=label, size=25, color="#3B82F6", shape="square", title=row['subject']))
                    exist_ids.add(nid)

            if not df_rels.empty:
                for _, r in df_rels.iterrows():
                    mnid = f"MAIL_{r['clue_id']}"
                    enid = f"ENT_{r['eid']}"
                    # ä»…æ·»åŠ ä¸ç°æœ‰çº¿ç´¢å…³è”çš„å®ä½“
                    if mnid in exist_ids:
                        if enid not in exist_ids:
                            color = "#F59E0B" if r['type'] == 'äººå' else "#10B981" if r[
                                                                                           'type'] == 'åœ°å' else "#8B5CF6"
                            nodes.append(Node(id=enid, label=r['name'], size=15, color=color, shape="dot"))
                            exist_ids.add(enid)
                        edges.append(Edge(source=mnid, target=enid, color="#E5E7EB"))

        # æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨è¯¦ç»†çš„ç‰©ç†å¼•æ“é…ç½®æ¥ç¡®ä¿å›¾è°±ç¨³å®šå’Œå±…ä¸­
        config = Config(
            width="100%",  # å®½åº¦è‡ªé€‚åº”å®¹å™¨
            height=700,    # å›ºå®šé«˜åº¦ (æ•´æ•°)ï¼Œé˜²æ­¢å¡Œé™·
            directed=True,
            nodeHighlightBehavior=True,
            highlightColor="#FCA5A5",
            collapsible=False,
            # å…³é”®é…ç½®ï¼šå¯ç”¨é€‚åº”è§†å›¾å’Œç‰©ç†ç¨³å®šåŒ–
            fit=True,
            physics={
                "enabled": True,
                "stabilization": {
                    "enabled": True,
                    "iterations": 1000, # é¢„è®¡ç®—1000æ¬¡å¸ƒå±€
                    "fit": True,        # ç¨³å®šåå¼ºåˆ¶é€‚åº”è§†å›¾
                    "updateInterval": 50,
                    "onlyDynamicEdges": False,
                },
                # è°ƒæ•´æ–¥åŠ›å‚æ•°ï¼Œè®©èŠ‚ç‚¹æ•£å¼€ï¼Œé¿å…é‡å 
                "barnesHut": {
                    "gravitationalConstant": -3000,
                    "centralGravity": 0.3,
                    "springLength": 95,
                    "springConstant": 0.04,
                    "damping": 0.09,
                    "avoidOverlap": 0.1
                },
                "minVelocity": 0.75
            }
        )

        # ä¿®å¤ï¼šç§»é™¤ key å‚æ•°
        if nodes:
            selected_id = agraph(nodes=nodes, edges=edges, config=config)
            if selected_id:
                st.session_state.selected_node_id = selected_id
        else:
            st.warning("å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— å…³è”èŠ‚ç‚¹")

    with cg2:
        st.markdown("#### è¯¦æƒ…é¢æ¿")
        with st.container(border=True):
            curr_id = st.session_state.selected_node_id
            if curr_id:
                details = get_node_detail(curr_id)
                if details:
                    st.caption(details['type'].upper())
                    st.markdown(f"**{details['title']}**")
                    st.divider()
                    for k, v in details['meta']:
                        st.write(f"**{k}:** {v}")
                    if details['body']:
                        st.markdown("---")
                        st.text_area("å†…å®¹æ‘˜è¦", details['body'], height=300)
            else:
                st.info("ğŸ‘ˆ ç‚¹å‡»å·¦ä¾§èŠ‚ç‚¹æŸ¥çœ‹")

# === ä¿æŒï¼šæŠ˜çº¿å›¾ (Line Chart) ===
with tab_time:
    st.markdown("#### ğŸ“… é‚®ä»¶æµé‡è¶‹åŠ¿")
    if not df_clues.empty:
        df_chart = df_clues.copy()
        df_chart['day'] = pd.to_datetime(df_chart['send_time']).dt.date
        df_grouped = df_chart.groupby(['day', 'org']).size().reset_index(name='count')

        fig_line = px.line(
            df_grouped,
            x='day',
            y='count',
            color='org',
            markers=True,
            labels={'day': 'æ—¥æœŸ', 'count': 'é‚®ä»¶æ•°é‡', 'org': 'æœºæ„'},
            title="æ¯æ—¥çº¿ç´¢æ•°é‡å˜åŒ–è¶‹åŠ¿"
        )
        fig_line.update_layout(hovermode="x unified")
        st.plotly_chart(fig_line, use_container_width=True)

        st.markdown("#### æ•°æ®æ˜ç»†")
        st.dataframe(df_clues[['send_time', 'org', 'source_email', 'subject']], use_container_width=True)

with tab_ent:

    st.dataframe(df_ents, use_container_width=True)
